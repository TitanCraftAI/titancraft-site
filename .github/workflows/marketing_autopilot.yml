name: Marketing â€” Autopilot SEO + Indexing

on:
  workflow_dispatch: {}
  schedule:
    - cron: '5 8 * * *'   # daily @ 08:05 UTC

permissions:
  contents: write

jobs:
  autopilot:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate sitemap & robots
        run: |
          python - <<'PY'
          import glob, os
          from urllib.parse import urljoin
          base = "https://titancraft.io/"
          htmls = [p for p in glob.glob("**/*.html", recursive=True)
                   if not p.startswith((".github/", "node_modules/"))]
          urls = [urljoin(base, p.replace("index.html","")) for p in htmls]
          urls = sorted(set([base] + urls))
          sm = '<?xml version="1.0" encoding="UTF-8"?>\n<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
          sm += "".join(f"<url><loc>{u}</loc></url>\n" for u in urls)
          sm += "</urlset>\n"
          open("sitemap.xml","w").write(sm)
          open("robots.txt","w").write("User-agent: *\nAllow: /\nSitemap: "+base+"sitemap.xml\n")
          PY

      - name: Commit & push SEO assets (safe)
        run: |
          set -euxo pipefail
          git config user.name "titan-bot"
          git config user.email "titan-bot@users.noreply.github.com"
          git add sitemap.xml robots.txt
          if git diff --cached --quiet; then
            echo "No SEO changes; skipping push."
            exit 0
          fi
          branch="${GITHUB_REF_NAME:-$(git rev-parse --abbrev-ref HEAD)}"
          git pull --rebase origin "$branch" || true
          git commit -m "chore(seo): auto-generate sitemap & robots"
          git push origin "HEAD:$branch"

      - name: Ping Google & Bing
        run: |
          curl -fsS "https://www.google.com/ping?sitemap=https://titancraft.io/sitemap.xml" || true
          curl -fsS "https://www.bing.com/ping?sitemap=https://titancraft.io/sitemap.xml" || true
